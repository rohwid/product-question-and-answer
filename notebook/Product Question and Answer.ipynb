{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Product QnA - Code Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/widiy/GitHub/learn-langchain/venv/lib/python3.9/site-packages/pinecone/index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "import os\n",
    "import pinecone\n",
    "import yaml\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# PDF loader\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# Text splitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Pinecone database\n",
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "# Word embedding\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "# QnA chains\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "# Agent imports\n",
    "from langchain.agents import initialize_agent\n",
    "\n",
    "# Tool imports\n",
    "from langchain.agents import Tool\n",
    "from langchain import SerpAPIWrapper\n",
    "\n",
    "# Prompt\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "# LLMChain\n",
    "from langchain import LLMChain\n",
    "\n",
    "# Memory\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its common when someone asks about the specific description or specification like \"What is Samsung Galaxy s23?\" or \"What the type of processor that Samsung Galaxy s23 use?\". So, here I asking to this QnA chatbot about \"Who is Tailor Swift?\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'Who is Tailor Swift?'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the credetial from params or `*.yml` file to makes it more easier to manage and accessible from other codes in this program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('../params/credentials.yml', 'r')\n",
    "credential = yaml.load(f, Loader=yaml.SafeLoader)\n",
    "f.close()\n",
    "\n",
    "OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY', credential['OPENAI_API_KEY'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load PDF, Pinecode Database"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the PDF file that already listed in params or `*.yml` file to make it more easier to manage and accessible from other codes in this program. The PDF document loaded with the Langchain module named `PyPDFLoader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 196 document(s) in your data\n",
      "There are 924 characters in your document\n"
     ]
    }
   ],
   "source": [
    "f = open('../params/app.yml', 'r')\n",
    "config = yaml.load(f, Loader=yaml.SafeLoader)\n",
    "f.close()\n",
    "\n",
    "loader = PyPDFLoader(config['PDF_SOURCE'][0])\n",
    "data = loader.load()\n",
    "\n",
    "print (f'You have {len(data)} document(s) in your data')\n",
    "print (f'There are {len(data[30].page_content)} characters in your document')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunk the data into more smaller data with Langchain module that called `RecursiveCharacterTextSplitter`. So, it can be more easier to embed and vectorize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now you have 200 documents\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(data)\n",
    "\n",
    "print (f'Now you have {len(texts)} documents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='2\\nTable of Contents\\nGetting started\\n5 D evice layout and functions\\n11 Char\\nging the battery\\n16\\n Nano-\\nSIM card and eSIM\\n18\\n T\\nurning the device on and off\\n19\\n Initial setup\\n20\\n Samsung ac\\ncount\\n21 T\\nransferring data from your previous \\ndevice (Smart Switch)\\n23\\n Und\\nerstanding the screen\\n31 Notif\\nication panel\\n33\\n S\\ncreen capture and screen record\\n34\\n E\\nntering text\\nApps and features\\n37 Install ing or uninstalling apps38\\n S P\\nen (Galaxy S23 Ultra)\\n54\\n \\nPhone\\n57\\n \\nContacts\\n59\\n \\nMessages\\n60\\n C\\namera\\n78\\n Gall\\nery\\n82\\n AR Z\\none\\n88\\n \\nBixby\\n89\\n Bix\\nby Vision\\n90\\n Multi \\nwindow (Using multiple apps \\nat once)\\n93\\n Samsung I\\nnternet94\\n Samsung \\nWallet\\n96\\n Samsung P\\nay\\n98\\n Samsung Health\\n99\\n Samsung Not\\nes\\n104\\n Samsung Memb\\ners\\n104\\n Samsung K\\nids\\n105\\n Samsung Gl\\nobal Goals\\n105\\n Samsung TV\\n Plus\\n105\\n Galax\\ny Shop\\n105\\n Galax\\ny Wearable\\n106\\n PENUP  (Galaxy S\\n23 Ultra)\\n106\\n C\\nalendar\\n107\\n R\\neminder  (Receiving notifications \\nfor to-do  items)\\n108\\n V\\noice Recorder\\n110\\n My\\n Files (Checking and managing \\nthe files)\\n110  Clock\\n110\\n \\nCalculator\\n111\\n Game Launcher\\n1\\n12\\n Game B\\nooster  (Configuring your \\ngaming environment)\\n113\\n Smar\\ntThings\\n113\\n Sharing c\\nontent\\n115\\n Music Shar\\ne\\n116\\n Smar\\nt View  (Mirroring on a TV \\nscreen)\\n117\\n Link \\nto Windows (Using the device \\nthrough a computer connection)', metadata={'source': '../data/user_manual.pdf', 'page': 1})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Pincone Database Key and environment from params or `*.yml` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY = os.environ.get('PINECONE_API_KEY', credential['PINECONE_API_KEY'])\n",
    "PINECONE_API_ENV = os.environ.get('PINECONE_API_ENV', credential['PINECONE_API_ENV'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the `OpenAIEmbeddings` module, the embedding module from OpenAI and use `text-embedding-ada-002` as model to embed all document that already chunked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(model='text-embedding-ada-002', openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initilaize the Pincone database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize pinecone\n",
    "pinecone.init(\n",
    "    api_key=PINECONE_API_KEY,  # find at app.pinecone.io\n",
    "    environment=PINECONE_API_ENV  # next to api key in console\n",
    ")\n",
    "\n",
    "index_name = credential['PINECONE_INDEX'] # put in the name of your pinecone index here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Warning**:</span> Skip or comment this step inside the line if you already upsert the data to Pincone Database.\n",
    "\n",
    "---\n",
    "\n",
    "Upsert the embedded data as vector to the Pincone or vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Pinecone.from_texts([t.page_content for t in texts], embeddings, index_name=index_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do query test by perform **similarity search** with `similarity_search` function that come up from `Pincone`, the Langchain module for data or vector query to the Pincone database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"{question}\"\n",
    "vectors = vectorstore.similarity_search(query)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check vector or data content as part of the similarity search result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectors[0].page_content[:450])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing on Pincone database and QnA Chain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and call OpenAI module to perform **Question and Answer** by chaining all `similarity_seach` result with the user question or the human input. If there a variable called `temperature` and set as to `0`, it means the model output mostly will be more deterministic and have small amount of variability. Because of this QnA chatbot will do specific task as product informer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0, model_name='text-davinci-003', openai_api_key=OPENAI_API_KEY)\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query the vector from the Pincone or vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pincone_index = pinecone.Index(index_name)\n",
    "text_field = \"text\"\n",
    "vectorstore = Pinecone(pincone_index, embeddings.embed_query, text_field)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform **similarity search** to the data or vector from the Pincone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f'{question}'\n",
    "vectors = vectorstore.similarity_search(query)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfom Question and Answer by chaining all `similarity_seach` result with the user question or the human input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I don't know.\n"
     ]
    }
   ],
   "source": [
    "from_pdf_qna = chain.run(input_documents=vectors, question=query)\n",
    "print(from_pdf_qna)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup environment for **SerpAPI** and set the certifi or SSL certified to grant access for python to access the internet and **perform Google Search with SerpAPI**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['SERPAPI_API_KEY'] = credential['SERPAPI_API_KEY']\n",
    "os.environ['REQUESTS_CA_BUNDLE'] = credential['REQUESTS_CA_BUNDLE_NOTEBOOK']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and call OpenAI module to perform search with langchain Agent through python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0, model_name = \"text-davinci-003\", openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and call SerpAPI library to perform Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = SerpAPIWrapper()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the Tool module from Langchain, the Search tool will be use by OpenAI model to perform search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_toolkit = [\n",
    "    Tool(\n",
    "        name = 'Search',\n",
    "        func=search.run,\n",
    "        description='useful for when you need to search google to answer questions about current events'\n",
    "    )\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the agent by load the tool, load the model that will perform search, and also set the verbose to `True`. So, every searching step can be shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = initialize_agent(search_toolkit, llm, agent='zero-shot-react-description', verbose=True, return_intermediate_steps=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform search and get the search result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out who Taylor Swift is\n",
      "Action: Search\n",
      "Action Input: \"Taylor Swift\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mTaylor Alison Swift is an American singer-songwriter. She is a prominent cultural figure, widely recognized for her genre-spanning discography, songwriting and artistic reinventions.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: Taylor Alison Swift is an American singer-songwriter. She is a prominent cultural figure, widely recognized for her genre-spanning discography, songwriting and artistic reinventions.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Taylor Alison Swift is an American singer-songwriter. She is a prominent cultural figure, widely recognized for her genre-spanning discography, songwriting and artistic reinventions.\n"
     ]
    }
   ],
   "source": [
    "response = agent({'input': f'{question}'})\n",
    "from_google = response['output']\n",
    "print(from_google)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform search and get the search result, but in this case the correlation with the product will be search too. In case of the user searching something that don't relate with the product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out who Taylor Swift is and what her connection is to the Samsung Galaxy s23\n",
      "Action: Search\n",
      "Action Input: \"Taylor Swift\" \"Samsung Galaxy s23\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mtaylor swift samsung s23 video · 7.6M views · Discover videos related to taylor swift samsung s23 video on TikTok.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to find out more about the video\n",
      "Action: Search\n",
      "Action Input: \"Taylor Swift Samsung Galaxy s23 video\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mtaylor swift samsung s23 video · 7.6M views · Discover videos related to taylor swift samsung s23 video on TikTok.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: Taylor Swift released a video for her song \"ME!\" in collaboration with Samsung Galaxy s23. The video has been viewed over 7.6 million times on TikTok.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Taylor Swift released a video for her song \"ME!\" in collaboration with Samsung Galaxy s23. The video has been viewed over 7.6 million times on TikTok.\n"
     ]
    }
   ],
   "source": [
    "response = agent({'input': f\"{question} and what is the correlation with {config['PRODUCT']}\"})\n",
    "from_product = response['output']\n",
    "print(from_product)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization with Prompt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the `PromptTemplate` module from Langchain to setup the instruction the model to perform summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "%INSTRUCTIONS:\n",
    "Please summarize the following piece of text.\n",
    "Convert the size metric to centimeters if exist.\n",
    "\n",
    "%TEXT:\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "# Create a LangChain prompt template that we can insert values to later\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=template,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also take the search result from PDF content and Google with SerpAPI. The output from the PDF file and also Google search result will be make the OpenAI know more about the current product that officialy released in February 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taylor Alison Swift is an American singer-songwriter. She is a prominent cultural figure, widely recognized for her genre-spanning discography, songwriting and artistic reinventions.\n",
      "\n",
      "Taylor Swift released a video for her song \"ME!\" in collaboration with Samsung Galaxy s23. The video has been viewed over 7.6 million times on TikTok.\n",
      "\n",
      "I don't know.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "confusing_text = \"\"\"{from_google}\n",
    "\n",
    "{from_product}\n",
    "\n",
    "{from_pdf_qna}\n",
    "\"\"\".format(\n",
    "    from_google = from_google.strip(),\n",
    "    from_product = from_product.strip(),\n",
    "    from_pdf_qna = from_pdf_qna.strip()\n",
    ")\n",
    "\n",
    "print(confusing_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the final prompt that contain instruction and text that need to be summarized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "%INSTRUCTIONS:\n",
      "Please summarize the following piece of text.\n",
      "Convert the size metric to centimeters if exist.\n",
      "\n",
      "%TEXT:\n",
      "Taylor Alison Swift is an American singer-songwriter. She is a prominent cultural figure, widely recognized for her genre-spanning discography, songwriting and artistic reinventions.\n",
      "\n",
      "Taylor Swift released a video for her song \"ME!\" in collaboration with Samsung Galaxy s23. The video has been viewed over 7.6 million times on TikTok.\n",
      "\n",
      "I don't know.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summarize_prompt = prompt.format(text=confusing_text)\n",
    "print(summarize_prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform summarization with OpenAI `text-davinci-003` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Taylor Swift is an American singer-songwriter who is well-known for her diverse music and creative reinventions. She recently released a video for her song \"ME!\" in collaboration with Samsung Galaxy s23, which has been viewed over 7.6 million times on TikTok.\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(temperature=0, model_name='text-davinci-003', openai_api_key=OPENAI_API_KEY)\n",
    "summarize_output = llm(summarize_prompt)\n",
    "print(summarize_output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot with Chain and Memory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a prompt to instruct the model to perform as a chatbot also take the summarizaion result previously, take the human input and get the chat history. The chatbot will automatically using chain that connecting or relating the current human input and the previous chat as chat history with `ConversationBufferMemory` or well known as memory. So, `LLMChain` is the library by langchain will use to perform this chain to connects the current chato or human input with the previous chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Setup prompt to perform as chatbot\n",
    "command = \"\"\"\n",
    "%INSTRUCTIONS:\n",
    "You are very helpful chatbot.\n",
    "Your goal are help the users to know more about the {product}.\n",
    "Please ignore if the question is not related with the Samsung Galaxy s23.\n",
    "This following piece of text are some information that you need to consider too.\n",
    "\n",
    "%TEXT: {text}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\", \"text\"],\n",
    "    template=command,\n",
    ")\n",
    "\n",
    "command_prompt = prompt.format(product=config['PRODUCT'], text=summarize_output)\n",
    "\n",
    "# Setup prompt to take human imput and chat history.\n",
    "chat = \"\"\"\n",
    "{chat_history}\n",
    "Human: {human_input}\n",
    "Chatbot:\"\"\"\n",
    "\n",
    "template = command_prompt + chat\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"human_input\"], \n",
    "    template=template\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "\n",
    "chat_chain = LLMChain(\n",
    "    llm=OpenAI(model_name='text-davinci-003', openai_api_key=credential['OPENAI_API_KEY']), \n",
    "    prompt=prompt, \n",
    "    verbose=True,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performs chat by taking the human input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "%INSTRUCTIONS:\n",
      "You are very helpful chatbot.\n",
      "Your goal are help the users to know more about the Samsung Galaxy s23.\n",
      "Please ignore if the question is not related with the Samsung Galaxy s23.\n",
      "This following piece of text are some information that you need to consider too.\n",
      "\n",
      "%TEXT: \n",
      "Taylor Swift is an American singer-songwriter who is well-known for her diverse music and creative reinventions. She recently released a video for her song \"ME!\" in collaboration with Samsung Galaxy s23, which has been viewed over 7.6 million times on TikTok.\n",
      "\n",
      "\n",
      "Human: Who is tailor swift?\n",
      "Chatbot:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Taylor Swift is an American singer-songwriter who is well-known for her diverse music and creative reinventions. She recently released a video for her song \"ME!\" in collaboration with Samsung Galaxy s23.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_chain.predict(human_input=f'{question}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Memory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the memory by asking about the previous chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "%INSTRUCTIONS:\n",
      "You are very helpful chatbot.\n",
      "Your goal are help the users to know more about the Samsung Galaxy s23.\n",
      "Please ignore if the question is not related with the Samsung Galaxy s23.\n",
      "This following piece of text are some information that you need to consider too.\n",
      "\n",
      "%TEXT: \n",
      "Taylor Swift is an American singer-songwriter who is well-known for her diverse music and creative reinventions. She recently released a video for her song \"ME!\" in collaboration with Samsung Galaxy s23, which has been viewed over 7.6 million times on TikTok.\n",
      "\n",
      "Human: Who is tailor swift?\n",
      "AI:  Taylor Swift is an American singer-songwriter who is well-known for her diverse music and creative reinventions. She recently released a video for her song \"ME!\" in collaboration with Samsung Galaxy s23.\n",
      "Human: What was I asked you about?\n",
      "Chatbot:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' You asked me about Taylor Swift.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_chain.predict(human_input=\"What was I asked you about?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
